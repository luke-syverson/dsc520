---
title: "Exercise 8.2"
author: "Luke Syverson"
date: "May 15, 2023"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### Work individually on this assignment. You are encouraged to collaborate on ideas and strategies pertinent to this assignment. Data for this assignment is focused on real estate transactions recorded from 1964 to 2016 and can be found in Housing.xlsx. Using your skills in statistical correlation, multiple regression, and R programming, you are interested in the following variables: Sale Price and several other possible predictors.

#### If you worked with the Housing dataset in previous week â€“ you are in luck, you likely have already found any issues in the dataset and made the necessary transformations. If not, you will want to take some time looking at the data with all your new skills and identifying if you have any clean up that needs to happen.

###Complete the following:

#### Explain any transformations or modifications you made to the dataset
```{r import}
library(readxl)
housing <- read_excel("data/week-7-housing.xlsx")
```
The ctyname variable is redundant by postalctyn, and contains null values. Being of higher granularity and lesser quality, it should be removed.
Sale_warning contains many nulls, and denotes an exception type that implies different sale behavior than the remaining observations. Subsetting records with sale warnings will lead to more meaningful analysis.
Year_renovated imputes 0 values for non-renovated houses. It may also be subsetted, as renovated houses will implicate another exceptional behavior.

Much of the qualitative data is rendered trivial by the unknown distinction between codes and values. Methods integrating the variables may create confounding influences per the potentially codified entries. Analysis of the categorical fields will yield results lacking in final interpretation.
Gross lot size vs. living lot size will also introduce variance per the disproportionate value of housing vs. land. Filtering to lot sizes that are arbitrarily some amount larger than the living size could induce selection bias, so the resulting noise should be accommodated for with additional analysis.
It will also be useful to sort observations by cities to bring meaning to the lat & lon fields.

The below instructions seem to warrant a more 'blind' analysis for Sale Price that takes into account the totality of the population's behavior. Following this exercise with mirrored analysis for a few subsets will yield additional insight into the housing market behavior across demographics, but this does not appear to be in the scope of the instruction.

Prop_type is entirely 'R', which is meaningless other than presumably indicating residential, and will be removed.
Lastly, spaces need to be removed from column names to perform modeling functions.

```{r filtering}
library(dplyr)
df <- housing %>% select(-'sale_warning', -'year_renovated', -'ctyname', -'prop_type')
colnames(df) <- gsub(" ", "_", colnames(df))
```

#### Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.
```{r subsets}
price_lot <- df %>% select(Sale_Price, sq_ft_lot)
predict_sale <- df %>% select(Sale_Price, year_built, bedrooms, sq_ft_lot, sitetype, building_grade)

pl_model <- lm(Sale_Price ~ sq_ft_lot, data = price_lot)
sale_model <- lm(Sale_Price ~ year_built + bedrooms + sq_ft_lot + building_grade + sitetype, data = predict_sale)
```
Living space, property usage/zoning & all geographic qualifiers except for zip5 are excluded per assumed multicollinearity with total lot size, sitetype and geography.
Sale date is excluded per the granularity, despite potential to quantify potential seasonality. Future analysis might aggregate the dates to the month level to increase generalization potential.
Bathrooms are excluded per the complexity of relationships between each possible combination. A weighed total of each type might be suited for future analysis.

#### Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?
```{r model_summary}
summary(pl_model)
summary(sale_model)
```
##### pl_model: Multiple R-squared:  0.01435,	Adjusted R-squared:  0.01428
The low R-statistics indicate little predictability of Sale Price given lot size.
##### sale_model Multiple R-squared:  0.1891,	Adjusted R-squared:  0.1885
The additional predictors fared slightly better, but lacked substantial improvement.
#### Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?
```{r stand_betas}
library("lm.beta")
lm.beta(sale_model)
```
It appears that building grade has the largest influences on the standard deviation of Sale Price, with building site types, year built, bedrooms and lot size following in descending order of magnitude. Building grade is the best predictor by far.
#### Calculate the confidence intervals for the parameters in your model and explain what the results indicate.
```{r CI}
confint(sale_model)
```
Most variables could be significant, and the range of the intervals other than site types and bedrooms ar fairly small. Site type seems to be confounding the significance of the model.
#### Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.
```{r ANOVA}
anova(pl_model, sale_model)
```
The p-value indicates a significant improvement from the original model.
#### Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.
```{r casewise}
cwise <- data.frame(
  residuals = residuals(sale_model),
  hat_values = hatvalues(sale_model),
  cooks_distance = cooks.distance(sale_model)
)
cwise
```
#### Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.
#### Use the appropriate function to show the sum of large residuals.
#### Which specific variables have large residuals (only cases that evaluate as TRUE)?
#### Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.
#### Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.
#### Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.
#### Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.
#### Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?